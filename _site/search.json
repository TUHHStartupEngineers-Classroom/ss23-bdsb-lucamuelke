[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "1 Source Code\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nbikes_tbl      <- read_excel(path = \"ds_data/01_bike_sales/01_raw_data/bikes.xlsx\")\norderlines_tbl <- read_excel(\"ds_data/01_bike_sales/01_raw_data/orderlines.xlsx\")\n\n#> New names:\n#> • `` -> `...1`\n\nbikeshops_tbl  <- read_excel(\"ds_data/01_bike_sales/01_raw_data/bikeshops.xlsx\")\n\nleft_join(orderlines_tbl, bikes_tbl, by = c(\"product.id\" = \"bike.id\"))\n\n\n\n  \n\n\nbike_orderlines_joined_tbl <- orderlines_tbl %>%\n  left_join(bikes_tbl, by = c(\"product.id\" = \"bike.id\")) %>%\n  left_join(bikeshops_tbl, by = c(\"customer.id\" = \"bikeshop.id\"))\n\nbike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%\n  mutate(total.price = price * quantity) %>%\n  separate(col    = location,\n           into   = c(\"city\", \"state\"),\n           sep    = \", \") %>%\n  separate(col    = category,\n           into   = c(\"category.1\", \"category.2\", \"category.3\"),\n           sep    = \" - \") %>%\n  select(-...1, -gender) %>%\n  select(-ends_with(\".id\")) %>%\n  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>% \n  select(order.id, contains(\"order\"), contains(\"model\"), contains(\"category\"),\n         price, quantity, total.price,\n         everything()) %>%\n  rename(bikeshop = name) %>%\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\"))\n\nlibrary(lubridate)\n\nsales_by_loc_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns\n  select(state, total_price) %>%\n  \n  # Grouping by year and summarizing sales\n  group_by(state) %>% \n  summarize(sales = sum(total_price)) %>%\n  \n  # Optional: Add a column that turns the numbers into a currency format \n  # (makes it in the plot optically more appealing)\n  # mutate(sales_text = scales::dollar(sales)) <- Works for dollar values\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\nsales_by_loc_tbl\n\n\n\n  \n\n\n# state                            sales sales_text  \n# <chr>                            <dbl> <chr>       \n#   1 Baden-Württemberg              6521090 6.521.090 € \n# 2 Bavaria                        6742819 6.742.819 € \n# 3 Berlin                         1128433 1.128.433 € \n# 4 Bremen                        10653499 10.653.499 €\n# 5 Hamburg                        3874756 3.874.756 € \n# 6 Hesse                          1558901 1.558.901 € \n\nsales_by_loc_tbl %>%\n  \n  # Setup canvas with the columns year (x-axis) and sales (y-axis)\n  ggplot(aes(x = state, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Again, we have to adjust it for euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by location\",\n    x = \"\", # Override defaults for x and y\n    y = \"Revenue\"\n  ) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n#> `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nsales_by_year_cat_1_tbl <- bike_orderlines_wrangled_tbl %>%\n  \n  # Select columns and add a year\n  select(order_date, total_price, state) %>%\n  mutate(year = year(order_date)) %>%\n  \n  # Group by and summarize year and main catgegory\n  group_by(year, state) %>%\n  summarise(sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  # Format $ Text\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#> `summarise()` has grouped output by 'year'. You can override using the\n#> `.groups` argument.\n\nsales_by_year_cat_1_tbl  \n\n\n\n  \n\n\n## # A tibble: 25 x 4\n##     year category_1      sales sales_text \n##    <dbl> <chr>           <dbl> <chr>      \n##  1  2015 E-Bikes       1599048 1.599.048 €\n##  2  2015 Gravel         663025 663.025 €  \n##  3  2015 Hybrid / City  502512 502.512 €  \n##  4  2015 Mountain      3254289 3.254.289 €\n##  5  2015 Road          3911408 3.911.408 €\n##  6  2016 E-Bikes       1916469 1.916.469 €\n\n\nsales_by_year_cat_1_tbl %>%\n  \n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = state)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Facet\n  facet_wrap(~ state) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and state\",\n    subtitle = \"\",\n    fill = \"Main category\" # Changes the legend name\n  ) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "library(httr)\nlibrary(jsonlite)\nlibrary(knitr)\n\nurl <- \"https://api.openweathermap.org/data/2.5/weather?q=New+York,US&units=metric&appid=de7f0078e2f8223f211a60f3ee3a2b94\"\nresponse <- GET(url)\n\nif (response$status_code == 200) {\n  weather_data <- fromJSON(rawToChar(response$content))\n  temperature <- weather_data$main$temp\n  humidity <- weather_data$main$humidity\n  wind_speed <- weather_data$wind$speed\n  cloud_cover <- weather_data$clouds$all\n  \n  weather_table <- data.frame(\n    \"Temperature\" = temperature,\n    \"Humidity\" = humidity,\n    \"Wind Speed\" = wind_speed,\n    \"Cloud Cover\" = cloud_cover\n  )\n\n  kable(weather_table, format = \"markdown\")\n} else {\n  cat(\"Error:\", response$status_code, response$reason)\n}\n\n\n\nTemperature\nHumidity\nWind.Speed\nCloud.Cover\n\n\n15.05\n42\n8.75\n0\n\n\n\n\n\n# WEBSCRAPING ----\n\n# 1.0 LIBRARIES ----\n\nlibrary(tidyverse) # Main Package - Loads dplyr, purrr, etc.\nlibrary(rvest)     # HTML Hacking & Web Scraping\nlibrary(xopen)     # Quickly opening URLs\nlibrary(jsonlite)  # converts JSON files to R objects\nlibrary(glue)      # concatenate strings\nlibrary(stringi)   # character string/text processing\nlibrary(xml2)\n\n\nhtml_bike_category  <- read_html(\"https://www.radon-bikes.de/roadbike-gravel/alu/bikegrid/\")\nbike_names_tbl        <- html_bike_category %>%\n  \n  html_nodes(css = \".a-heading--small\")# %>%\n\nnames <- xml_text(bike_names_tbl)\nnames <- names[-c(36, 37)]\nnames <- names[-5]\nnames <- names[-4]\n\nbike_price_tbl        <- html_bike_category %>%\n  \n  html_nodes(css = \".m-bikegrid__price--active\")\n\nprices <- xml_text(bike_price_tbl)\nlibrary(stringr)\nprices <- str_remove(prices, \" ₤\")\nprices <- str_remove(prices, \"₤\")\nprices <- str_remove(prices, \"₤ \")\nprices <- prices[prices != \"\"]\nlibrary(readr)\nprices_numeric <- sapply(prices, parse_number)\nprices_numeric <- unname(prices_numeric)\n\noutput <- rbind(names, prices_numeric)\nkable(output, format = \"markdown\")\n\n\n\nnames\nR1 DISC TIAGRA\nR1 DISC 105\nR1 DISC ULTEGRA\n\n\nprices_numeric\n1299\n1499\n1799"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "library(vroom)\nlibrary(knitr)\ncol_types <- list(\n  id = col_character(),\n  type = col_character(),\n  number = col_character(),\n  country = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  abstract = col_character(),\n  title = col_character(),\n  kind = col_character(),\n  num_claims = col_double(),\n  filename = col_character(),\n  withdrawn = col_double()\n)\n\npatent_tbl <- vroom(\n  file       = \"patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#> Warning: The following named parsers don't match the column names: type,\n#> number, country, abstract, title, kind, filename, withdrawn\n\nassignee_tbl <- vroom(\n  file       = \"assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#> Warning: The following named parsers don't match the column names: number,\n#> country, date, abstract, title, kind, num_claims, filename, withdrawn\n\npatent_assignee_tbl <- vroom(\n  file       = \"patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#> Warning: The following named parsers don't match the column names: id, type,\n#> number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\npatent_tbl <- vroom(\n  file       = \"patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#> Warning: The following named parsers don't match the column names: type,\n#> number, country, abstract, title, kind, filename, withdrawn\n\nmerged_df <- merge(patent_tbl, patent_assignee_tbl, by.x = \"id\", by.y = \"patent_id\")\nmerged_df_2 <- merge(merged_df, assignee_tbl, by.x = \"assignee_id\", by.y = \"id\")\n\n\nmost_common_ids <- sort(table(with(merged_df_2, merged_df_2[(type == 2), ])$assignee_id), decreasing=TRUE)[1:10]\n\nmost_common_ids <- names(most_common_ids)\ntop_list <- lapply(most_common_ids, function(x) with(assignee_tbl, array(organization[id == x])))\n\n\n\naugust <- with(merged_df_2, merged_df_2[(date >= \"2014-08-01\" & date <= \"2014-08-31\" & type == 2), ])\nmost_common_august <- names(sort(table(merged_df_2$organization), decreasing=TRUE)[1:10])\n\n\nuspc_tbl <- vroom(\n  file       = \"uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#> Warning: The following named parsers don't match the column names: id, type,\n#> number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\nmerged_df_3 <- merge(merged_df_2, uspc_tbl, by.x = \"id\", by.y = \"patent_id\")\n\n#> Warning: One or more parsing issues, call `problems()` on your data frame for details,\n#> e.g.:\n#>   dat <- vroom(...)\n#>   problems(dat)\n\nmost_common_world <- names(sort(table(patent_assignee_tbl$assignee_id), decreasing=TRUE)[1:10])\ntop_uspto <- lapply(most_common_world, function(x) with(merged_df_3, array(mainclass_id[assignee_id == x])))\ntop_uspto <- unlist(top_uspto)\ntop_uspto <- names(sort(table(top_uspto), decreasing=TRUE)[1:5])\n\n\nkable(top_list, format = \"markdown\")\n\n\n\n\n\n\nx\n\n\nInternational Business Machines Corporation\n\n\n\n\n\n\nx\n\n\nMicrosoft Corporation\n\n\n\n\n\n\nx\n\n\nGoogle Inc.\n\n\n\n\n\n\nx\n\n\nQUALCOMM Incorporated\n\n\n\n\n\n\nx\n\n\nApple Inc.\n\n\n\n\n\n\nx\n\n\nGeneral Electric Company\n\n\n\n\n\n\nx\n\n\nHewlett-Packard Development Company, L.P.\n\n\n\n\n\n\nx\n\n\nAT&T INTELLECTUAL PROPERTY I, L.P.\n\n\n\n\n\n\nx\n\n\nIntel Corporation\n\n\n\n\n\n\nx\n\n\nGM Global Technology Operations LLC\n\n\n\n\n\nkable(most_common_august, format = \"markdown\")\n\n\n\nx\n\n\n\nInternational Business Machines Corporation\n\n\nSamsung Electronics Co., Ltd.\n\n\nCanon Kabushiki Kaisha\n\n\nSony Corporation\n\n\nMicrosoft Corporation\n\n\nGoogle Inc.\n\n\nKabushiki Kaisha Toshiba\n\n\nQUALCOMM Incorporated\n\n\nLG Electronics Inc.\n\n\nPanasonic Corporation\n\n\n\n\nkable(top_uspto, format = \"markdown\")\n\n\n\nx\n\n\n\n257\n\n\n455\n\n\n370\n\n\n348\n\n\n709"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Challenge 1\n\nlibrary(ggplot2)\nlibrary(tidyverse)\ncovid_data_tbl <- read_csv(\"owid-covid-data.csv\")\n\n#> Rows: 311447 Columns: 67\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsubset_germany <- subset(covid_data_tbl, location == \"Germany\" | location == \"United Kingdom\" | location == \"France\" | location == \"Spain\" | location == \"United States\")\n\nggplot(subset_germany, aes(x = date, y = total_cases, color = location)) + geom_line() + theme_dark() + scale_x_date(date_labels = \"%b-%Y\", date_breaks = \"1 month\") + labs(title = \"COVID-19 confirmed cases worldwide\", subtitle = \"As of 19/04/2022\", y = \"Cumulative Cases\") + scale_y_continuous(labels = function(x) paste0(x/1000000,\"M\")) + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), axis.title.x=element_blank(), legend.position=\"bottom\") + scale_color_brewer(palette=\"Set1\")\n\n#> Warning: Removed 83 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\nChallenge 2\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(scales)\ncovid_data_tbl <- read_csv(\"owid-covid-data.csv\")\n\n#> Rows: 311447 Columns: 67\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nreduced <- covid_data_tbl[which(covid_data_tbl$date == \"2022-04-16\"), ]\n\n\nreduced <- distinct(mutate(reduced, location = case_when(location == \"United Kingdom\" ~ \"UK\", location == \"United States\" ~ \"USA\", location == \"Democratic Republic of Congo\" ~ \"Democratic Republic of the Congo\", TRUE ~ location)))\n\nreduced$mortality_rate <- reduced$total_deaths / reduced$population\n\nworld <- ggplot2::map_data(\"world\")\n\ntotal <- merge(world, reduced, by.x = \"region\", by.y = \"location\")\n\nggplot(total) + geom_map(aes(map_id = region, fill = mortality_rate), map = world) + theme_dark() + expand_limits(x = world$long, y = world$lat) + scale_fill_gradient(low = \"red\", high = \"black\", labels = percent_format()) + labs(title = \"Confirmed COVID-19 deaths relative to the size of the population\", subtitle = \"Around 6.2 Million confimed COVID-19 deaths worldwide\", caption = \"Date: 04/16/2022\", fill = \"Mortality Rate\") + theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) + borders(\"world\", colour = \"black\", size = 0.1)"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  }
]